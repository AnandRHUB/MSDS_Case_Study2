---
title: "CaseStudy2"
author: "Anand R and Shantanu G"
date: "December 2, 2018"
output: 
  html_document:
    keep_md: yes
---
####1. Setting environment and parameters
```{r setup, include=FALSE, echo = TRUE}
# Load necessary libraries
#install.packages("caret")
#install.packages(mlr)
library(mlr)
library(caret)
library(ggplot2)
library(dplyr)
library(knitr)
library(kableExtra)
library(plyr)
library(tidyverse)

```

### Background
We are from DDSAnalytics a talent management company specializing in talent management solutions. We do solutions for workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition).


### About our client

Our client Anheuser-Busch is one of the top beer producers in the world with the following vitals to boast. Head quartered in St. Louis MO, the company brews more than 100 brands of beers with flagship brands Budweiser and Bud Light.
Budweiser has an issue with employee attrition and improving employee morale to gain competitive edge in its industry. We are conducting an analysis using the existing employee data.


### Pitfalls of High attrition
1. Losing talented employee with the company's business knowledge
2. Replacing with new employee costs
3. Replaced employee needs to be trained in business
4. Recruiting costs

### Analysis
The goal is to ascertain the HR problem of employee attrition and how we can predict the rate through Machine Learning model.
We were supplied with data set of 1170 rows of data with 34 columns, which we will use to
do the exploratory data analysis and predic the top 3 attributes that would affect the attrition.

  The following steps will be done in that order.

1. Conduct exploratory data analysis
2. Create training and test data sets
3. Build knn classification model to train the training data provided.
4. Minimise the top 10 columns that comprise the model to predict with accuracy
5. Run it thru again to check if they can give top 3 variables that affect attrition
6. Evaluation of the model - we will do several iterations to come up with changing k values
7. Conclusion - draw inferences from the model

### 1. Exploratory Data Analysis

```{r}

setwd("C:/Anands/DS_HW/GIT/ExampleProject/MSDS6306-CaseStudy2")

training <- read.csv("DataFiles/CaseStudy2-data.csv")

#Exploratory data Analysis

# Split training and test sets for our analysis and model
samplesize_attrition = nrow(training)

train_perc = .70
train_indices = sample(seq(1,samplesize_attrition,length = samplesize_attrition),train_perc*samplesize_attrition)
dfTrain = training[train_indices,]
dfVal  = training[-train_indices,]

#Write CSV file to destination folder

write.csv(dfTrain, file ="dfTrain.csv")
write.csv(dfVal, file="dfVal.csv")

str(dfTrain)

summary(dfTrain)

dim(dfVal)

dim(dfTrain)

# Removing columns which do not vary as well as the random column

dfTrain$Over18 <- NULL
dfTrain$EmployeeCount <- NULL
dfTrain$StandardHours <- NULL
dfTrain$EmployeeNumber <- NULL
dfTrain$Rand <- NULL

# Convert following columns as factors
dfTrain$Education <- as.factor(dfTrain$Education)
dfTrain$EnvironmentSatisfaction <- as.factor(dfTrain$EnvironmentSatisfaction)
dfTrain$JobInvolvement <- as.factor(dfTrain$JobInvolvement)
dfTrain$JobLevel <- as.factor(dfTrain$JobLevel)
dfTrain$JobSatisfaction <- as.factor(dfTrain$JobSatisfaction)
dfTrain$PerformanceRating <- as.factor(dfTrain$PerformanceRating)
dfTrain$RelationshipSatisfaction <- as.factor(dfTrain$RelationshipSatisfaction)
dfTrain$StockOptionLevel <- as.factor(dfTrain$StockOptionLevel)
dfTrain$TrainingTimesLastYear <- as.factor(dfTrain$TrainingTimesLastYear)
dfTrain$WorkLifeBalance <- as.factor(dfTrain$WorkLifeBalance)

# finally assigning the data to a data frame which has clean data for model.

# This optimizes the training model 
trainc <- trainControl(method="repeatedcv", number=5, repeats=5)

#Creates a model for the learning machine which can be used for predictions on new data.

fitknn=train(Attrition~., dfTrain, method="knn", trControl=trainc)

impvars=varImp(fitknn, scale=FALSE)

impvars

```

#### Elimination of columns not affecting the outcome for training set

We eliminate the columns which have no variance.
a. Over18 has no variability, all are Y.
b. EmployeeCount has no variability - all are 1.
c. StandardHours has no variability - all are 80.
d. Employee Number is an identifier.
e. Rand is a column generated and unused in the model

#### Data Exploration

#### Report column NA's

* No NA columns found in data

* Scatter plots on finding correlation between attributes

####1.  Correlation between Monthly Income, Monthly Rate and Hourly Rate

####2.  Correlation between Education, EnvironmentSatisfacton, JobInvolvement

```{r}

#This function searches through a correlation matrix and returns a vector of 
corr_matrix=cor(dfTrain[sapply(dfTrain, is.numeric)])

#integers corresponding to columns to remove to reduce pair-wise correlations.

highCorr = findCorrelation(corr_matrix, cutoff=0.50)

highCorr

#Using Pairs comparison to find correlation
#Correlation diagram

pairs (~ MonthlyIncome + MonthlyRate + HourlyRate ,
       data = dfTrain ,col=4, main="fig1: Correlation-Income, MonthlyRate and Hourly rate")

pairs (~EducationField + EnvironmentSatisfaction #+ Gender,
      ,data = dfTrain ,col=6, main="fig2: Correlation-EducationField and Environment")

```

* The Correlation diagrams do not show any significant correlation in the scatter diagram and are good candidates for model.

#### Finding the core columns for the final knn classification model.
```{r}
# This optimizes the training model 
trainc <- trainControl(method="repeatedcv", number=5, repeats=5)

#Creates a model for the learning machine which can be used for predictions on new data.

fitknn=train(Attrition~., dfTrain, method="knn", trControl=trainc)

impvars=varImp(fitknn, scale=FALSE)

impvars

```

#### Scatter plots association with Attrition
1. fig 3: Scatter plot on Income and Work Life Balance with Attrition
2. fig 4: Scatter plot on Education and Department with Attrition

```{r}

gg1 <- ggplot(dfTrain, aes(x=WorkLifeBalance,                                     y=MonthlyIncome, color = Attrition)) + 
       geom_point() + 
         labs(subtitle="fig3:Income vs Work Life Balance", 
          y="Income", 
          x="WorkLife Balance", 
          title="Scatterplot")

plot(gg1)

# Attrition per Education & department 
gg2 <- ggplot(dfTrain, aes(x=Education, y=Department, color = Attrition)) + 
  geom_point() + 
    labs(subtitle="fig4:Education and Department", 
       y="Department", 
       x="Education", 
       title="Scatterplot")

plot(gg2)
```

#### Plots to show the Attrition based on monthly income and Job levels.
Following are violin plots that show attrition
1. Monthly income
2. Monthly income and Job level.

```{r}
#GG histogram/density between monthly income, and attrition

gg3 <- ggplot(dfTrain, aes(x=MonthlyIncome, fill = Attrition)) +
  geom_histogram(binwidth = 500, aes(y=..density..)) +
  geom_density(alpha=.2,fill="#FF6666") +
  labs(title="fig5:MonthlyIncome and Attrition",
       y="Density",
       x="MonthlyIncome",
       subtitle="Histogram / Density") 

plot(gg3)

#scatter plot between monthly income, JobLevel and attrition
gg4 <- ggplot(training, aes(x=training$MonthlyIncome, y=training$JobLevel, color = Attrition)) + 
  geom_point() + 
    labs(subtitle="fig6:Monthly Income and Joblevel",
       y="JobLevel",
       x="Monthly Income",
       title="Scatterplot")
plot(gg4)

```
#Various plots to show Attrition and explanatory variables such as Income, Age, Years of service, Marital status and Years in service with company
1. Bar plot on Years at company and Income level
2. Box plot on Age and attrition
3. Scatter plot on Years at company and Age
4. Box plot on Years Since Last promotion and Attrition
5. Jitter plot on Marital Status and Attrition
```{r}
# Plot to show Years at company and Income attrition.

grad <- scales::seq_gradient_pal("blue", "blue")(seq(0,1,length.out=100))

ggplot(dfTrain, aes(x=reorder(dfTrain$YearsAtCompany, -dfTrain$YearsAtCompany), y=dfTrain$MonthlyIncome, fill=Attrition)) +
  geom_bar(stat='identity', position='dodge') +
  labs(title=" fig7: Years at company and income level", x="Years in service", y="Monthly Income") +
  theme(plot.title = element_text(hjust=0.5), axis.text.x=element_text(angle=90, size=7), 
        legend.position="none") +     
  scale_fill_manual(values=grad)

#boxplot between Age and attrition
gg5 <- ggplot(training, aes(x=Attrition, y=Age, color = Attrition)) + 
  geom_boxplot() + 
    labs(subtitle="fig8:Age and Attrition",
       y="Age",
       x="Attrition",
       title="Box Plot")
plot(gg5)

# Scatter plot on Years at company and Age
ggplot(dfTrain, aes(x=YearsAtCompany, y=Age, color=YearsAtCompany)) + 
  geom_point(size=1.3, na.rm=TRUE) + 
 # geom_smooth(method=lm, na.rm=TRUE, se=FALSE, color="blue") +
  labs(title="fig9:Years at company and Age", x="Years at Company", y="Age of Employee") +
  theme(plot.title = element_text(hjust=0.5), legend.position="none") +
  scale_color_gradient(low = "#ffbf00", high = "blue")

# Years Since Last promotion and Attrition
gg6 <- ggplot(dfTrain, aes(y=YearsSinceLastPromotion, x=Attrition, color = Attrition)) + 
  geom_boxplot() + 
    labs(subtitle="fig10:Years Since Last promotion and Attrition",
       y="Years Since last promotion",
       x="Attrition",
       title="Box Plot")
plot(gg6)

# Jitter plot on Marital Status and Attrition

gg7 <- ggplot(dfTrain, aes(x=MaritalStatus, y=Attrition, color = Attrition)) + 
  geom_jitter() + 
    labs(subtitle="fig11:Marital Status and Attrition",
       y="Attrition",
       x="Marital Status",
       title="Jitter Plot")
plot(gg7)

```

###Final model for prediction
The model is built based on outcome from ROC which predicts the columns that are correlated with attrition history provided.

```{r}
# Now the actual model and prediction

kval <- 5
# defining model data frames Training and Test

dftr <- data.frame(dfTrain[, c(1,2,3,14,17,18,21,25,26,29,30,32)]) %>% droplevels()

dfts <- data.frame(dfVal[, c(1,2,3,16,19,20,24,29,30,33,34,36)]) %>% droplevels()

dftr$JobLevel <- as.integer(dftr$JobLevel)
dftr$MaritalStatus <- as.integer(dftr$MaritalStatus)
dftr$OverTime <- as.integer(dftr$OverTime)
dftr$StockOptionLevel <- as.integer(dftr$StockOptionLevel)

dfts$JobLevel <- as.integer(dfts$JobLevel)
dfts$MaritalStatus <- as.integer(dfts$MaritalStatus)
dfts$OverTime <- as.integer(dfts$OverTime)
dfts$StockOptionLevel <- as.integer(dfts$StockOptionLevel)


# Appropriate model after getting the results
results = class::knn(dftr[,c(2,4,5,6,7,8,9,10,11,12)], dfts[,c(2,4,5,6,7,8,9,10,11,12)], dftr$Attrition, k=kval)
# Predict the data thus trained with test set
dfts$PredictedAttrition <- results
  
table(dfts$Attrition,dfts$PredictedAttrition)
  
#Confushion Matrix creation
cm <- caret::confusionMatrix(table(dfts$Attrition,dfts$PredictedAttrition))

cmhd<-head(cm$table)

kable(cmhd, row.names=FALSE, caption="Table 1: Confushion Matrix showing prediction") %>% kable_styling(bootstrap_options="bordered")

cm

model_accuracy_knn=sum(dfts$PredictedAttrition == dfts$Attrition)/nrow(dfts)

model_accuracy_knn

#Around 83 percent

dfPreds <- data.frame(dfts$ID,dfts$PredictedAttrition)
colnames(dfPreds) <- c("ID","PredictedAttrition")
write.csv(dfPreds, file="dfPreds.csv")

```

###Conclusion:

Model accuracy is .833 ie 83.3% of the times the data will predict attrition of employee correctly.
Further it is only good with the current data set and trends within the data. If the data composition changes over time, the program would compute a different set that would predict the outcome differently.

=======
###Prepared by Anand Rajan and Shantanu Godbole from DDSAnalytics
=======
